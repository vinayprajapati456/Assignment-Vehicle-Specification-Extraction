{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbeaa390",
   "metadata": {
    "id": "fbeaa390"
   },
   "source": [
    "## 1) Install required packages\n",
    "\n",
    "Run this cell to install Python packages used in the notebook. This installs PyMuPDF, SentenceTransformers, FAISS (CPU), and OpenAI client library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e8b266",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48e8b266",
    "outputId": "a74035d4-1689-40e6-8b89-d432aedf96f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install -q pymupdf sentence-transformers faiss-cpu openai nbformat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba71cdb",
   "metadata": {
    "id": "aba71cdb"
   },
   "source": [
    "## 2) Upload the sample service manual (PDF)\n",
    "\n",
    "Use the file upload widget to upload the `sample-service-manual 1.pdf`. The notebook will use the uploaded file as `PDF_PATH`. If you previously placed the file in `/mnt/data/`, you may also set the path manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b13dde2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "b13dde2d",
    "outputId": "83a1e23c-d4b9-4773-e590-8ea224637832"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-b6dd09f5-b16f-46a5-bb7c-92ac5d5655b4\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-b6dd09f5-b16f-46a5-bb7c-92ac5d5655b4\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sample-service-manual 1.pdf to sample-service-manual 1.pdf\n",
      "Using PDF: sample-service-manual 1.pdf\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "# Determine the PDF path\n",
    "if uploaded:\n",
    "    PDF_PATH = list(uploaded.keys())[0]\n",
    "else:\n",
    "    # Fallback if the file already exists in the environment\n",
    "    PDF_PATH = \"/mnt/data/sample-service-manual 1.pdf\"\n",
    "print(\"Using PDF:\", PDF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad7157c",
   "metadata": {
    "id": "9ad7157c"
   },
   "source": [
    "## 3) Pipeline overview (what each section does)\n",
    "\n",
    "- **Text extraction:** Read PDF pages using PyMuPDF (fitz) and extract textual content.\n",
    "- **Chunking:** Split text into overlapping chunks to preserve local context for retrieval.\n",
    "- **Embeddings:** Convert chunks to dense vectors using SentenceTransformers (all-MiniLM-L6-v2).\n",
    "- **Vector DB (FAISS):** Store embeddings and perform fast nearest-neighbor search.\n",
    "- **Retrieval:** Given a user query, retrieve the top-k most relevant chunks.\n",
    "- **Extraction:** Use OpenAI LLM to extract structured JSON if `OPENAI_API_KEY` is present; otherwise use regex heuristics.\n",
    "\n",
    "This notebook is intentionally verbose and educational, with clear explanations and example outputs at each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d753fe19",
   "metadata": {
    "id": "d753fe19"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 4) Pipeline code (detailed and commented)\n",
    "import os, re, json, time\n",
    "from typing import List, Tuple\n",
    "from collections import namedtuple\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Optional OpenAI\n",
    "import openai\n",
    "\n",
    "# Configuration (you can edit these)\n",
    "PDF_PATH = globals().get(\"PDF_PATH\", \"/mnt/data/sample-service-manual 1.pdf\")\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"   # small, fast, and good for retrieval\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 200\n",
    "TOP_K = 5\n",
    "FAISS_DIR = \"/content/faiss_index_optionB\"\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "Chunk = namedtuple(\"Chunk\", [\"id\", \"text\", \"page_no\"])\n",
    "\n",
    "# Text extraction--\n",
    "# Returns a list of (page_no, text) tuples extracted from the PDF.\n",
    "# Uses PyMuPDF (fitz) because it is fast and preserves textual order reasonably well.\n",
    "def extract_text_from_pdf(pdf_path: str) -> List[Tuple[int, str]]:\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"PDF not found at {pdf_path}. Upload it first.\")\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = []\n",
    "    for i, page in enumerate(doc):\n",
    "        txt = page.get_text(\"text\")\n",
    "        pages.append((i+1, txt if txt else \"\"))\n",
    "    return pages\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    # Normalize whitespace and remove stray control characters\n",
    "    s = s.replace(\"\\x0c\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "# Chunking strategy\n",
    "def chunk_pages(pages, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    chunks = []\n",
    "    cid = 0\n",
    "    for page_no, text in pages:\n",
    "        text = clean_text(text)\n",
    "        if not text:\n",
    "            continue\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = min(start + chunk_size, len(text))\n",
    "            piece = text[start:end].strip()\n",
    "            chunks.append(Chunk(cid, piece, page_no))\n",
    "            cid += 1\n",
    "            if end == len(text):\n",
    "                break\n",
    "            start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "# Build embeddings and FAISS index\n",
    "def build_embeddings_and_index(chunks, model_name=EMBED_MODEL):\n",
    "    print(\"Loading embedding model:\", model_name)\n",
    "    model = SentenceTransformer(model_name)\n",
    "    texts = [c.text for c in chunks]\n",
    "    print(f\"Encoding {len(texts)} chunks...\")\n",
    "    emb = model.encode(texts, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True)\n",
    "    print(\"Embeddings shape:\", emb.shape)\n",
    "    index = faiss.IndexFlatIP(emb.shape[1])\n",
    "    index.add(emb)\n",
    "    os.makedirs(FAISS_DIR, exist_ok=True)\n",
    "    faiss.write_index(index, os.path.join(FAISS_DIR, \"index.faiss\"))\n",
    "\n",
    "    # Save metadata and embeddings\n",
    "    meta = [{\"id\": c.id, \"page_no\": c.page_no, \"text\": c.text} for c in chunks]\n",
    "    with open(os.path.join(FAISS_DIR, \"meta.json\"), \"w\", encoding=\"utf8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "    np.save(os.path.join(FAISS_DIR, \"emb.npy\"), emb)\n",
    "    print(\"Saved FAISS index and metadata to\", FAISS_DIR)\n",
    "    return model, index, meta\n",
    "\n",
    "def load_index_and_meta():\n",
    "    idx = faiss.read_index(os.path.join(FAISS_DIR, \"index.faiss\"))\n",
    "    with open(os.path.join(FAISS_DIR, \"meta.json\"), \"r\", encoding=\"utf8\") as f:\n",
    "        meta = json.load(f)\n",
    "    emb = np.load(os.path.join(FAISS_DIR, \"emb.npy\"))\n",
    "    model = SentenceTransformer(EMBED_MODEL)\n",
    "    return model, idx, meta, emb\n",
    "\n",
    "# Retrieval\n",
    "def retrieve(query: str, model, index, meta, top_k=TOP_K):\n",
    "    q_emb = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if idx < 0 or idx >= len(meta):\n",
    "            continue\n",
    "        m = meta[idx]\n",
    "        results.append({\"score\": float(score), \"page_no\": m[\"page_no\"], \"text\": m[\"text\"]})\n",
    "    return results\n",
    "\n",
    "# Heuristic extraction (fallback)\n",
    "def heuristic_extract(retrieved_chunks):\n",
    "    results = []\n",
    "    # find torque-like patterns and fluid capacities, part numbers patterns too\n",
    "    torque_re = re.compile(r\"(\\d{1,4}(?:\\.\\d+)?(?:\\s*-\\s*\\d{1,4}(?:\\.\\d+)?)?)\\s*(Nm|N·m|N m|lb-ft|lb·ft|ft-lb|in-lb)\", re.I)\n",
    "    capacity_re = re.compile(r\"([0-9]+(?:\\.[0-9]+)?)\\s*(L|litre|litres|ml|cc)\", re.I)\n",
    "    part_re = re.compile(r\"(?:Part|P/N|PN|Part Number)[:\\s]*([A-Za-z0-9\\-_/]+)\", re.I)\n",
    "\n",
    "    for c in retrieved_chunks:\n",
    "        t = c[\"text\"]\n",
    "        p = c[\"page_no\"]\n",
    "        # torque matches\n",
    "        for m in torque_re.finditer(t):\n",
    "            val = m.group(1)\n",
    "            unit = m.group(2)\n",
    "            snippet = t[max(0,m.start()-100):m.end()+50].strip()\n",
    "            comp = snippet.split(\"\\n\")[-1][:120]\n",
    "            results.append({\n",
    "                \"component\": comp.strip() or \"UNKNOWN\",\n",
    "                \"spec_type\": \"Torque\",\n",
    "                \"value\": val.strip(),\n",
    "                \"unit\": unit.strip(),\n",
    "                \"context\": snippet,\n",
    "                \"source_page\": p\n",
    "            })\n",
    "        # capacity matches\n",
    "        for m in capacity_re.finditer(t):\n",
    "            val = m.group(1)\n",
    "            unit = m.group(2)\n",
    "            snippet = t[max(0,m.start()-60):m.end()+30].strip()\n",
    "            comp = snippet.split(\"\\n\")[-1][:120]\n",
    "            results.append({\n",
    "                \"component\": comp.strip() or \"UNKNOWN\",\n",
    "                \"spec_type\": \"Capacity\",\n",
    "                \"value\": val.strip(),\n",
    "                \"unit\": unit.strip(),\n",
    "                \"context\": snippet,\n",
    "                \"source_page\": p\n",
    "            })\n",
    "        # part numbers\n",
    "        for m in part_re.finditer(t):\n",
    "            pn = m.group(1)\n",
    "            snippet = t[max(0,m.start()-60):m.end()+30].strip()\n",
    "            results.append({\n",
    "                \"component\": snippet.split(\"\\n\")[-1][:120].strip() or \"UNKNOWN\",\n",
    "                \"spec_type\": \"Part Number\",\n",
    "                \"value\": pn.strip(),\n",
    "                \"unit\": \"\",\n",
    "                \"context\": snippet,\n",
    "                \"source_page\": p\n",
    "            })\n",
    "    # dedupe\n",
    "    seen = set()\n",
    "    dedup = []\n",
    "    for r in results:\n",
    "        key = (r[\"component\"], r[\"spec_type\"], r[\"value\"], r[\"unit\"])\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        dedup.append(r)\n",
    "    return dedup\n",
    "\n",
    "# OpenAI LLM extraction\n",
    "def build_prompt(query, retrieved_chunks):\n",
    "    header = (\"You are an assistant that extracts vehicle specifications from manual text.\\n\"\n",
    "              \"Given the query and the supplied text chunks, extract relevant specs in JSON.\\n\"\n",
    "              \"Return an array of objects with fields: component, spec_type, value, unit, context, source_page.\\n\\n\")\n",
    "    qtxt = f\"Query: {query}\\n\\n\"\n",
    "    chunk_texts = \"\"\n",
    "    for i, c in enumerate(retrieved_chunks):\n",
    "        chunk_texts += f\"--- CHUNK {i+1} | PAGE {c['page_no']} ---\\n{c['text']}\\n\\n\"\n",
    "    rules = (\"Rules:\\n1) Return ONLY a JSON array (no explanation).\\n\"\n",
    "             \"2) Use empty string for unknown fields.\\n3) Normalize units when obvious (e.g., 'Nm').\\n\")\n",
    "    prompt = header + qtxt + rules + chunk_texts + \"\\nReturn the JSON array now.\"\n",
    "    return prompt\n",
    "\n",
    "def llm_extract_openrouter(prompt):\n",
    "    import os, json, re\n",
    "    import requests\n",
    "\n",
    "    api_key = os.environ.get(\"OPEN_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENROUTER_API_KEY not set in environment.\")\n",
    "\n",
    "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": OPENAI_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 800\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"HTTP-Referer\": \"http://localhost\",\n",
    "        \"X-Title\": \"Spec-Extractor\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract model text output\n",
    "    content = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    # try to parse JSON in response\n",
    "    m = re.search(r\"(\\[.*\\])\", content, flags=re.S)\n",
    "    if m:\n",
    "        return json.loads(m.group(1))\n",
    "    try:\n",
    "        return json.loads(content)\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"OpenRouter returned non-JSON content.\") from e\n",
    "\n",
    "# Build and query top-level functions\n",
    "def build_pipeline(pdf_path=PDF_PATH):\n",
    "    pages = extract_text_from_pdf(pdf_path)\n",
    "    print(f\"Extracted {len(pages)} pages.\")\n",
    "    chunks = chunk_pages(pages)\n",
    "    print(f\"Created {len(chunks)} chunks.\")\n",
    "    model, index, meta = build_embeddings_and_index(chunks)\n",
    "    return model, index, meta\n",
    "\n",
    "def query_pipeline(query, use_openai=True, top_k=TOP_K):\n",
    "    model, index, meta, emb = load_index_and_meta()\n",
    "    retrieved = retrieve(query, model, index, meta, top_k=top_k)\n",
    "    # When using OpenAI and a key is present, call LLM and return parsed JSON\n",
    "    if use_openai and os.environ.get(\"OPENAI_API_KEY\"):\n",
    "        prompt = build_prompt(query, retrieved)\n",
    "        try:\n",
    "            extracted = llm_extract_openrouter(prompt)\n",
    "            return extracted\n",
    "        except Exception as e:\n",
    "            print(\"OpenAI extraction failed; falling back to heuristic. Error:\", e)\n",
    "            return heuristic_extract(retrieved)\n",
    "    else:\n",
    "        return heuristic_extract(retrieved)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f5f98c",
   "metadata": {
    "id": "12f5f98c"
   },
   "source": [
    "## 5) Build the FAISS index (run once)\n",
    "\n",
    "This step extracts text from the uploaded PDF, chunks it, creates embeddings, and builds a FAISS index. It may take a few minutes depending on the document length and runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d9f98f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170,
     "referenced_widgets": [
      "5ec90e197a5647fc9cca71ab5aa05e05",
      "9c7d30e3a6a841609641cb863e426655",
      "f7e7fc83658d4199985e444acc79964d",
      "7ccf20bcaf124c1394933ec01a04eab1",
      "70655a379d094a1db856d8f55e24dad0",
      "d25143b6cd764d50b3c9b717f52488ff",
      "999874ef9c8d4bb6b3bfff08f0099391",
      "ab160b98175f43bdac1aa67a890f4226",
      "c5ac8073e6bf4d38b24e0116ceee1c91",
      "93945e0fea654e7aa4a89e9356d001d7",
      "1e1475c81278464d9188c7dbfc5dbe88"
     ]
    },
    "id": "4d9f98f5",
    "outputId": "1313d735-f2b8-4888-b938-079fd19c7a04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 852 pages.\n",
      "Created 1604 chunks.\n",
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Encoding 1604 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec90e197a5647fc9cca71ab5aa05e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (1604, 384)\n",
      "Saved FAISS index and metadata to /content/faiss_index_optionB\n",
      "Index built. Number of saved chunks (meta): 1604\n"
     ]
    }
   ],
   "source": [
    "# Build the pipeline: extract -> chunk -> embed -> FAISS\n",
    "model, index, meta = build_pipeline(PDF_PATH)\n",
    "print(\"Index built. Number of saved chunks (meta):\", len(meta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec11aa7",
   "metadata": {
    "id": "dec11aa7"
   },
   "source": [
    "## 6) Query the pipeline\n",
    "\n",
    "Provide sample queries below. If you want to use OpenAI for better extraction, set API key in the environment first using:\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "```\n",
    "\n",
    "If the key is missing the notebook will fall back to the heuristic extractor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pWwE9pnzSkD7",
   "metadata": {
    "id": "pWwE9pnzSkD7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPEN_API_KEY'] = 'XYZ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b1b3355",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b1b3355",
    "outputId": "962d224a-85aa-452f-e8f2-25c9085a0316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Torque for brake caliper bolts\n",
      "[\n",
      "  {\n",
      "    \"component\": \"Brake caliper guide pin bolts\",\n",
      "    \"spec_type\": \"Torque\",\n",
      "    \"value\": \"33\",\n",
      "    \"unit\": \"Nm\",\n",
      "    \"context\": \"\",\n",
      "    \"source_page\": \"652\"\n",
      "  },\n",
      "  {\n",
      "    \"component\": \"Brake caliper support bracket bolts\",\n",
      "    \"spec_type\": \"Torque\",\n",
      "    \"value\": \"150\",\n",
      "    \"unit\": \"Nm\",\n",
      "    \"context\": \"\",\n",
      "    \"source_page\": \"652\"\n",
      "  },\n",
      "  {\n",
      "    \"component\": \"Brake caliper anchor plate bolts\",\n",
      "    \"spec_type\": \"Torque\",\n",
      "    \"value\": \"250\",\n",
      "    \"unit\": \"Nm\",\n",
      "    \"context\": \"\",\n",
      "    \"source_page\": \"636\"\n",
      "  },\n",
      "  {\n",
      "    \"component\": \"Brake caliper flow bolt\",\n",
      "    \"spec_type\": \"Torque\",\n",
      "    \"value\": \"35\",\n",
      "    \"unit\": \"Nm\",\n",
      "    \"context\": \"\",\n",
      "    \"source_page\": \"636\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Example queries\n",
    "q = \"Torque for brake caliper bolts\"\n",
    "print('Query:', q)\n",
    "res = query_pipeline(q, use_openai=True)\n",
    "import json\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1c178",
   "metadata": {
    "id": "7db1c178"
   },
   "source": [
    "## 7) Save and download results\n",
    "\n",
    "Save the last query results to a JSON file and download it to your machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9feab04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "d9feab04",
    "outputId": "3b69e75e-729f-4945-c920-93d6cd60c641"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_1d3e6e34-26bf-4ecd-b1f5-316a1906e104\", \"specs_output_optionB.json\", 2355)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save last result (res) to JSON and provide download link\n",
    "from google.colab import files\n",
    "with open('specs_output_optionB.json', 'w') as f:\n",
    "    json.dump(res, f, indent=2)\n",
    "files.download('specs_output_optionB.json')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
